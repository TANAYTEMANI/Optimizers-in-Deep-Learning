# Optimizers-in-Deep-Learning
An optimizer is a function or an algorithm that modifies the attributes of the neural network, such as weights and learning rate. Thus, it helps in reducing the overall loss and improve the accuracy.

Types of Optimizers:
<ol>
  <li>Gradient Descent</li>
  <li>Stochastic Gradient Descent</li>
  <li>Momentum</li>
  <li>Mini-Batch Gradient Descent</li>
  <li>Adagrad</li>
  <li>RMSProp</li>
  <li>AdaDelta</li>
  <li>Adam</li>
</ol>

<h4>Stochastic Gradient Descent</h4>
In Stochastic Gradient Descent a few samples are selected randomly instead of the whole data set for each iteration.<br>
It usually takes a higher number of iterations to reach the minima, because of its randomness in its descent.<br><br><br>

![image](https://user-images.githubusercontent.com/82306595/213449306-4cf3b840-63f6-4243-aa20-6ab50eeff8db.png)
